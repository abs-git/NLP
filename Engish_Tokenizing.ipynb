{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Engish_Tokenizing",
      "provenance": [],
      "collapsed_sections": [
        "bkSYqvxmtWLc"
      ],
      "authorship_tag": "ABX9TyOHr9C8fJlN6WcSfDmKCtIv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "85316fece54147d39601ac264def1b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ee4267b2dbc457aabf02b756d701a5a",
              "IPY_MODEL_6c53dd43393c4d11bf4ffcaf31aa1396",
              "IPY_MODEL_10b1f368ce99464d92c304a1bd402d3e"
            ],
            "layout": "IPY_MODEL_9162195989bb492990a21f4d62cdacf8"
          }
        },
        "4ee4267b2dbc457aabf02b756d701a5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0f46dec632d4578bbf73502b1e08f5d",
            "placeholder": "​",
            "style": "IPY_MODEL_025a9f5d3c1d43399c1bf8e237b0284f",
            "value": "100%"
          }
        },
        "6c53dd43393c4d11bf4ffcaf31aa1396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_441f2e76aba440a4b8127f5eb0e05c1c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a42685e7dfa456b88a5dd3b6a593336",
            "value": 3
          }
        },
        "10b1f368ce99464d92c304a1bd402d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ab534eca511474da0476d9c93f81f70",
            "placeholder": "​",
            "style": "IPY_MODEL_21935ee3984b4785bb72dc7aa726625d",
            "value": " 3/3 [00:00&lt;00:00, 43.58it/s]"
          }
        },
        "9162195989bb492990a21f4d62cdacf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0f46dec632d4578bbf73502b1e08f5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "025a9f5d3c1d43399c1bf8e237b0284f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "441f2e76aba440a4b8127f5eb0e05c1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a42685e7dfa456b88a5dd3b6a593336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ab534eca511474da0476d9c93f81f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21935ee3984b4785bb72dc7aa726625d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abs-git/NLP/blob/main/Engish_Tokenizing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OjauShm6ko0",
        "outputId": "59ace6de-c4c7-4588-cbcd-437a605636b3"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data load"
      ],
      "metadata": {
        "id": "bkSYqvxmtWLc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCNswxRbAiW2"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric"
      ],
      "metadata": {
        "id": "ezq5vc-ZV5rV"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
        "\n",
        "task = \"cola\"\n",
        "\n",
        "dataset = load_dataset(\"glue\", task)\n",
        "metric = load_metric(\"glue\", task)"
      ],
      "metadata": {
        "id": "Isiv0plWIH46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "85316fece54147d39601ac264def1b71",
            "4ee4267b2dbc457aabf02b756d701a5a",
            "6c53dd43393c4d11bf4ffcaf31aa1396",
            "10b1f368ce99464d92c304a1bd402d3e",
            "9162195989bb492990a21f4d62cdacf8",
            "f0f46dec632d4578bbf73502b1e08f5d",
            "025a9f5d3c1d43399c1bf8e237b0284f",
            "441f2e76aba440a4b8127f5eb0e05c1c",
            "2a42685e7dfa456b88a5dd3b6a593336",
            "1ab534eca511474da0476d9c93f81f70",
            "21935ee3984b4785bb72dc7aa726625d"
          ]
        },
        "outputId": "554935a0-d601-4b84-a537-fca98cfa64e9"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85316fece54147d39601ac264def1b71"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n",
        "print()\n",
        "print(dataset[\"train\"])\n",
        "print()\n",
        "print(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "sDmxAGbbI4GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "YGOlnHpLJgTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data split and save\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/NLP'\n",
        "\n",
        "train_data = []\n",
        "for i, data in enumerate(dataset['train']):\n",
        "  train_data.append(dataset['train'][i]['sentence'])\n",
        "\n",
        "with open(path + '/glue_train.txt', 'w', encoding='utf-8') as f:\n",
        "  for sen in train_data:\n",
        "    f.write(sen + '\\n')\n",
        "\n",
        "\n",
        "test_data = []\n",
        "for i, data in enumerate(dataset['test']):\n",
        "  test_data.append(dataset['test'][i]['sentence'])\n",
        "\n",
        "with open(path + '/glue_test.txt', 'w', encoding='utf-8') as f:\n",
        "  for sen in test_data:\n",
        "    f.write(sen + '\\n')\n",
        "\n"
      ],
      "metadata": {
        "id": "zku5h3EeKkT_"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "svymZe03tbAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data load\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/NLP'\n",
        "\n",
        "\n",
        "with open(path + '/glue_train.txt', 'r') as f:\n",
        "  train_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(train_sentences):\n",
        "  train_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "with open(path + '/glue_test.txt', 'r') as f:\n",
        "  test_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(test_sentences):\n",
        "  test_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "print(len(train_sentences))\n",
        "print(len(test_sentences))\n",
        "\n",
        "print(train_sentences[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU3TN76GWIaV",
        "outputId": "eed5d7af-ef6d-48af-cc0b-9bc1b0accac0"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8551\n",
            "1063\n",
            "[\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\", \"One more pseudo generalization or I'm giving up.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "v8BvQtLoKXdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4d1a4e-f326-4a62-9d60-4400761db944"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Word Tokenization\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import spacy\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "sentences = train_sentences[:5]\n",
        "\n",
        "# nltk word_tokenize\n",
        "nltk_word_tokens = []\n",
        "for sen in sentences:\n",
        "  tokens = word_tokenize(sen)\n",
        "  nltk_word_tokens.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# nltk word punct\n",
        "nltk_punct_tokens = []\n",
        "for sen in sentences:\n",
        "  tokens = WordPunctTokenizer().tokenize(sen)               # 구두점을 별도로 분류한다.\n",
        "  nltk_punct_tokens.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# nltk Tree bank\n",
        "nltk_tree_bank_tokens = []\n",
        "for sen in sentences:\n",
        "  tokens = TreebankWordTokenizer().tokenize(sen)               # 하이픈(-)은 하나의 단어로, 아포스트로피(')는 접어로 취급한다.\n",
        "  nltk_tree_bank_tokens.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# spacy tokenize\n",
        "spacy_tokens = []\n",
        "spacy_en = spacy.load('en')\n",
        "for sen in sentences:\n",
        "  tokens = [tok.text for tok in spacy_en.tokenizer(sen)]\n",
        "  spacy_tokens.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# tensorflow keras\n",
        "keras_tokens = []\n",
        "for sen in sentences:\n",
        "  tokens = text_to_word_sequence(sen)               # 자동으로 lower가 진행되고 구두점(,.!)을 제거하지만, 아포스트로피(')는 보존한다.\n",
        "  keras_tokens.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# torch\n",
        "torch_tokenizer = get_tokenizer(\"basic_english\")\n",
        "torch_tokens = []\n",
        "for sen in sentences:\n",
        "  tokens = torch_tokenizer(sen)\n",
        "  torch_tokens.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Raw sentence :          \", sentences[0])\n",
        "print(\"nltk word tokens :      \", nltk_word_tokens[0])\n",
        "print(\"nltk punct tokens :     \", nltk_punct_tokens[0])\n",
        "print(\"nltk tree bank tokens : \", nltk_tree_bank_tokens[0])\n",
        "print(\"spacy tokens :          \", spacy_tokens[0])\n",
        "print(\"keras tokens :          \", keras_tokens[0])\n",
        "print(\"torch okens :           \", torch_tokens[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha7BQ_VbU7KH",
        "outputId": "fdf65da0-91ef-4e21-8e22-32753913e2fa"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw sentence :           Our friends won't buy this analysis, let alone the next one we propose.\n",
            "nltk word tokens :       ['Our', 'friends', 'wo', \"n't\", 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "nltk punct tokens :      ['Our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "nltk tree bank tokens :  ['Our', 'friends', 'wo', \"n't\", 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "spacy tokens :           ['Our', 'friends', 'wo', \"n't\", 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "keras tokens :           ['our', 'friends', \"won't\", 'buy', 'this', 'analysis', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose']\n",
            "torch okens :            ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "corpus = ' '.join(sentences)\n",
        "\n",
        "nltk_sent_tokens = sent_tokenize(corpus)            # 문장의 끝이 아닌 마침표(.)를 구분해서 분할\n",
        "\n",
        "\n",
        "print(\"corpus           : \", corpus)\n",
        "print(\"nltk sent tokens : \", nltk_sent_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQYqg9cnU_j3",
        "outputId": "dc872883-fd71-4f0c-ba6b-addf05076218"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus           :  Our friends won't buy this analysis, let alone the next one we propose. One more pseudo generalization and I'm giving up. One more pseudo generalization or I'm giving up. The more we study verbs, the crazier they get. Day by day the facts are getting murkier.\n",
            "nltk sent tokens :  [\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\", \"One more pseudo generalization or I'm giving up.\", 'The more we study verbs, the crazier they get.', 'Day by day the facts are getting murkier.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subword Tokenization\n",
        "\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "\n",
        "# BPE (Byte Pair Encoding)\n",
        "class BPE():\n",
        "  def __init__(self):\n",
        "    self.vocab = []\n",
        "\n",
        "\n",
        "  def make_dict(self, corpus):     # 코퍼스(corpus)를 읽어들여 글자(character)단위로 분리 후 딕셔너리(dict) 생성\n",
        "                                   # corpus는 sentence가 담긴 list\n",
        "    subword_dict = defaultdict(int)\n",
        "\n",
        "    for sen in corpus:\n",
        "      tokens = WordPunctTokenizer().tokenize(sen)\n",
        "\n",
        "\n",
        "      for token in tokens:\n",
        "        subword_dict[\" \".join(list(token)).lower() + \" </w>\"] = 1      # </w> 문자는 subword의 전/후면 위치를 구분 가능하도록 함.\n",
        "\n",
        "        self.vocab.extend(list(token.lower()))\n",
        "\n",
        "    return dict(subword_dict)\n",
        "\n",
        "\n",
        "  def get_pairs(self, subword_dict):          # dictionary 내의 bi-gram pair를 생성하고 빈도를 counting 한다. \n",
        "    \n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in subword_dict.items():\n",
        "      word_list = word.split()\n",
        "\n",
        "      for i in range(len(word_list) - 1):\n",
        "        pairs[(word_list[i], word_list[i+1])] += freq\n",
        "\n",
        "    return dict(pairs)\n",
        "\n",
        "\n",
        "  def merge_dict(self, pairs, subword_dict):\n",
        "\n",
        "    merged_subword_dict = defaultdict(int)\n",
        "    best_pair = max(pairs, key = pairs.get)   # bi-gram pair dictionary에 가장 많이 등장하는 \n",
        "\n",
        "    # print(best_pair)\n",
        "    for word, freq in subword_dict.items():\n",
        "      \n",
        "      if \" \".join(best_pair) in word:\n",
        "        merged_word = word.replace(\" \".join(best_pair), \"\".join(best_pair))\n",
        "\n",
        "        merged_subword_dict[merged_word] = subword_dict[word]    \n",
        "      else:\n",
        "        merged_subword_dict[word] = subword_dict[word]\n",
        "\n",
        "    return dict(merged_subword_dict)\n",
        "\n",
        "\n",
        "  def get_vocab(self, subword_dict):\n",
        "\n",
        "    temp_vocab = []\n",
        "    for word, freq in subword_dict.items():\n",
        "      word_list = word.split()\n",
        "\n",
        "      temp_vocab.extend(word_list)\n",
        "\n",
        "    self.vocab = list(set(temp_vocab) | set(self.vocab))\n",
        "\n",
        "    return self.vocab\n",
        "    \n"
      ],
      "metadata": {
        "id": "PtdQ2-wXU9UP"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "byte_pair_encoder = BPE()\n",
        "\n",
        "subword_dict = byte_pair_encoder.make_dict(sentences)\n",
        "print(subword_dict)\n",
        "print()\n",
        "\n",
        "for i in range(20):\n",
        "  pairs = byte_pair_encoder.get_pairs(subword_dict)\n",
        "  subword_dict = byte_pair_encoder.merge_dict(pairs, subword_dict)\n",
        "  vocab = byte_pair_encoder.get_vocab(subword_dict)\n",
        "\n",
        "print(subword_dict)\n",
        "print()\n",
        "print(vocab)\n",
        "print(len(vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1SgGspa8Ghy",
        "outputId": "601553d7-cc23-4707-ce80-37d39e16a085"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'o u r </w>': 1, 'f r i e n d s </w>': 1, 'w o n </w>': 1, \"' </w>\": 1, 't </w>': 1, 'b u y </w>': 1, 't h i s </w>': 1, 'a n a l y s i s </w>': 1, ', </w>': 1, 'l e t </w>': 1, 'a l o n e </w>': 1, 't h e </w>': 1, 'n e x t </w>': 1, 'o n e </w>': 1, 'w e </w>': 1, 'p r o p o s e </w>': 1, '. </w>': 1, 'm o r e </w>': 1, 'p s e u d o </w>': 1, 'g e n e r a l i z a t i o n </w>': 1, 'a n d </w>': 1, 'i </w>': 1, 'm </w>': 1, 'g i v i n g </w>': 1, 'u p </w>': 1, 'o r </w>': 1, 's t u d y </w>': 1, 'v e r b s </w>': 1, 'c r a z i e r </w>': 1, 't h e y </w>': 1, 'g e t </w>': 1, 'd a y </w>': 1, 'b y </w>': 1, 'f a c t s </w>': 1, 'a r e </w>': 1, 'g e t t i n g </w>': 1, 'm u r k i e r </w>': 1}\n",
            "\n",
            "{'o u r</w>': 1, 'f r ie nd s</w>': 1, 'w on</w>': 1, \"' </w>\": 1, 't</w>': 1, 'b u y</w>': 1, 'th is</w>': 1, 'a n al y s is</w>': 1, ', </w>': 1, 'l e t</w>': 1, 'al one</w>': 1, 'th e</w>': 1, 'ne x t</w>': 1, 'one</w>': 1, 'w e</w>': 1, 'p r o p o s e</w>': 1, '. </w>': 1, 'm o re</w>': 1, 'p s e ud o </w>': 1, 'ge ne r al i z a ti on</w>': 1, 'a nd </w>': 1, 'i </w>': 1, 'm </w>': 1, 'g i v i ng</w>': 1, 'u p </w>': 1, 'o r</w>': 1, 's t ud y</w>': 1, 'v e r b s</w>': 1, 'c r a z ie r</w>': 1, 'th e y</w>': 1, 'ge t</w>': 1, 'd a y</w>': 1, 'b y</w>': 1, 'f a c t s</w>': 1, 'a re</w>': 1, 'ge t ti ng</w>': 1, 'm u r k ie r</w>': 1}\n",
            "\n",
            "['g', 'y</w>', 'p', 's', 'c', 'one</w>', 'u', 'nd', 'z', 'f', '.', 't', 'i', 'on</w>', 'w', 'ti', 'k', 'n', 't</w>', 'h', 'v', 'o', 'a', 'r</w>', 're</w>', 'x', 's</w>', 'th', 'ng', 'is</w>', \"'\", 'ge', 'y', 'e</w>', 'ng</w>', 'ne', 'e', 'on', 'm', 'l', 'b', 'ud', 'ie', ',', 'd', 'al', 'r', '</w>']\n",
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data load\n",
        "# reference : https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb#scrollTo=UtFQqK3tmp7G\n",
        "# !wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
      ],
      "metadata": {
        "id": "uw37v7Ix-BIP"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SentencePiece (Unigram Language Model Tokenizer)\n",
        "\n",
        "import sentencepiece as spm\n",
        "import re\n",
        "\n",
        "train_data = './botchan.txt'\n",
        "\n",
        "# command setting\n",
        "templates = '--input={} --model_prefix={} --vocab_size={} --model_type={}'\n",
        "\n",
        "vocab_size = 5000\n",
        "prefix = './sentencepiece_prefix'\n",
        "model_type = 'bpe'\n",
        "\n",
        "cmd = templates.format(train_data, prefix, vocab_size, model_type)\n",
        "\n",
        "\n",
        "# train\n",
        "spm.SentencePieceTrainer.Train(cmd)\n",
        "\n",
        "\n",
        "# model load\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('{}.model'.format(prefix))\n",
        "\n",
        "with open('{}.vocab'.format(prefix), encoding='utf-8') as f:      # vocabrary 정보\n",
        "  vocab_info = [info.strip().split(\"\\t\") for info in f]\n",
        "\n",
        "\n",
        "# test\n",
        "tokens = sp.encode_as_pieces(test_sentences[0])    # Tokenizing\n",
        "tokens_IDs = sp.encode_as_ids(test_sentences[0])\n",
        "\n",
        "sen = sp.decode_pieces(tokens)                    # Detokenizing\n",
        "sen_from_IDs = sp.decode_ids(tokens_IDs)\n",
        "\n",
        "print(test_sentences[0])\n",
        "print(tokens)\n",
        "print(tokens_IDs)\n",
        "print(sen)\n",
        "print(sen_from_IDs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYlvlyQIobNg",
        "outputId": "bbd988d5-38ca-48d8-f182-1111f07b7d51"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bill whistled past the house.\n",
            "['▁B', 'ill', '▁wh', 'ist', 'led', '▁past', '▁the', '▁house', '.']\n",
            "[180, 157, 105, 261, 447, 1563, 9, 278, 4951]\n",
            "Bill whistled past the house.\n",
            "Bill whistled past the house.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Huggingface tokenizer\n",
        "\n",
        "# !pip install tokenizers\n",
        "import os\n",
        "from tokenizers import CharBPETokenizer, BertWordPieceTokenizer, SentencePieceBPETokenizer, ByteLevelBPETokenizer\n",
        "\n",
        "# model load\n",
        "Basic_tokenizer = CharBPETokenizer()\n",
        "SP_tokenizer = SentencePieceBPETokenizer()\n",
        "BL_tokenizer = ByteLevelBPETokenizer()\n",
        "BWP_tokenizer = BertWordPieceTokenizer(strip_accents= False, lowercase= False)  # strip_accents는 학습 시와 load 시 동일해야한다.\n",
        "\n",
        "\n",
        "# data load\n",
        "data_path = '/content/gdrive/MyDrive/Colab Notebooks/NLP'\n",
        "\n",
        "# params\n",
        "corpus_file = [data_path + '/glue_train.txt']\n",
        "vocab_size = 32000\n",
        "limit_alphabet = 5000\n",
        "output_path = data_path + '/output'\n",
        "min_freq = 10\n",
        "\n",
        "\n",
        "# train\n",
        "Basic_tokenizer.train(files = corpus_file,\n",
        "                      vocab_size = vocab_size,\n",
        "                      min_frequency = min_freq,\n",
        "                      limit_alphabet = limit_alphabet,\n",
        "                      show_progress = True\n",
        "                      )\n",
        "\n",
        "SP_tokenizer.train(files = corpus_file,\n",
        "                      vocab_size = vocab_size,\n",
        "                      min_frequency = min_freq,\n",
        "                      limit_alphabet = limit_alphabet,\n",
        "                      show_progress = True\n",
        "                      )\n",
        "\n",
        "BL_tokenizer.train(files = corpus_file,\n",
        "                      vocab_size = vocab_size,\n",
        "                      min_frequency = min_freq,\n",
        "                      show_progress = True\n",
        "                      )\n",
        "\n",
        "BWP_tokenizer.train(files = corpus_file,\n",
        "                      vocab_size = vocab_size,\n",
        "                      min_frequency = min_freq,\n",
        "                      limit_alphabet = limit_alphabet,\n",
        "                      show_progress = True\n",
        "                      )\n",
        "\n",
        "\n",
        "# model(vocab) save\n",
        "# Basic_tokenizer.save_model(data_path)\n",
        "# SP_tokenizer.save_model(data_path)\n",
        "# BL_tokenizer.save_model(data_path)\n",
        "# BWP_tokenizer.save_model(data_path)\n",
        "\n",
        "\n",
        "# model(vocab) load\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# test\n",
        "Basic_output = Basic_tokenizer.encode(test_sentences[0])\n",
        "SP_output = SP_tokenizer.encode(test_sentences[0])\n",
        "BL_output = BL_tokenizer.encode(test_sentences[0])\n",
        "BWP_output = BWP_tokenizer.encode(test_sentences[0])\n",
        "\n",
        "basic_decoded = Basic_tokenizer.decode(Basic_output.ids)\n",
        "SP_decoded = SP_tokenizer.decode(SP_output.ids)\n",
        "BL_decoded = BL_tokenizer.decode(BL_output.ids)\n",
        "BWP_decoded = BWP_tokenizer.decode(BWP_output.ids)\n",
        "\n",
        "\n",
        "print(\"input   : {}\".format(test_sentences[0]))\n",
        "print()\n",
        "\n",
        "print(\"basic tokens  : {}\".format(Basic_output.tokens))\n",
        "print(\"basic id      : {}\".format(Basic_output.ids))\n",
        "print(\"basic offsets : {}\".format(Basic_output.offsets))\n",
        "print(\"basic decode  : {}\".format(basic_decoded))\n",
        "print()\n",
        "\n",
        "print(\"SP tokens  : {}\".format(SP_output.tokens))\n",
        "print(\"SP id      : {}\".format(SP_output.ids))\n",
        "print(\"SP offsets : {}\".format(SP_output.offsets))\n",
        "print(\"SP decode  : {}\".format(SP_decoded))\n",
        "print()\n",
        "\n",
        "print(\"BL tokens  : {}\".format(BL_output.tokens))\n",
        "print(\"BL id      : {}\".format(BL_output.ids))\n",
        "print(\"BL offsets : {}\".format(BL_output.offsets))\n",
        "print(\"BL decode  : {}\".format(BL_decoded))\n",
        "print()\n",
        "\n",
        "print(\"BWP tokens  : {}\".format(BWP_output.tokens))\n",
        "print(\"BWP id      : {}\".format(BWP_output.ids))\n",
        "print(\"BWP offsets : {}\".format(BWP_output.offsets))\n",
        "print(\"BWP decode  : {}\".format(BWP_decoded))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtoHBvnLgw9P",
        "outputId": "99b86c05-0602-49bc-b605-21759d62345b"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input   : Bill whistled past the house.\n",
            "\n",
            "basic tokens  : ['Bill</w>', 'wh', 'ist', 'led</w>', 'p', 'ast</w>', 'the</w>', 'house</w>', '.</w>']\n",
            "basic id      : [232, 189, 724, 362, 68, 1123, 153, 689, 106]\n",
            "basic offsets : [(0, 4), (5, 7), (7, 10), (10, 13), (14, 15), (15, 18), (19, 22), (23, 28), (28, 29)]\n",
            "basic decode  : Bill whistled past the house .\n",
            "\n",
            "SP tokens  : ['▁Bill', '▁wh', 'ist', 'led', '▁p', 'ast', '▁the', '▁house', '.']\n",
            "SP id      : [197, 215, 374, 777, 105, 388, 89, 746, 11]\n",
            "SP offsets : [(0, 4), (4, 7), (7, 10), (10, 13), (13, 15), (15, 18), (18, 22), (22, 28), (28, 29)]\n",
            "SP decode  : Bill whistled past the house.\n",
            "\n",
            "BL tokens  : ['Bill', 'Ġwh', 'ist', 'led', 'Ġp', 'ast', 'Ġthe', 'Ġhouse', '.']\n",
            "BL id      : [567, 374, 501, 641, 274, 528, 259, 898, 13]\n",
            "BL offsets : [(0, 4), (4, 7), (7, 10), (10, 13), (13, 15), (15, 18), (18, 22), (22, 28), (28, 29)]\n",
            "BL decode  : Bill whistled past the house.\n",
            "\n",
            "BWP tokens  : ['Bill', 'whis', '##tle', '##d', 'past', 'the', 'house', '.']\n",
            "BWP id      : [215, 952, 2187, 91, 2055, 140, 743, 13]\n",
            "BWP offsets : [(0, 4), (5, 9), (9, 12), (12, 13), (14, 18), (19, 22), (23, 28), (28, 29)]\n",
            "BWP decode  : Bill whistled past the house.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subword text encoder (tensorflow)\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# model load\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_sentences, target_vocab_size = 30000)\n",
        "\n",
        "tokens_sen = tokenizer.encode(test_sentences[0])\n",
        "original_sen = tokenizer.decode(tokens_sen)\n",
        "\n",
        "print(tokenizer.subwords[:10])\n",
        "print()\n",
        "\n",
        "print(test_sentences[0])\n",
        "print(tokens_sen)\n",
        "print(original_sen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w68b-jBoFwNP",
        "outputId": "1dcc408c-0a0a-4487-b9c6-97684ca21260"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the_', 'to_', 'The_', 'a_', 'I_', 'that_', 'is_', ', ', 'John_', 'of_']\n",
            "\n",
            "Bill whistled past the house.\n",
            "[23, 3032, 2527, 1, 359, 7681]\n",
            "Bill whistled past the house.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word tokenizing for Neural Network using keras\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences_1 = sentences[:3]\n",
        "sentences_2 = sentences[3:]\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")     # 토큰화 되지 않은 문장이 입력될 때 <OOV> 인덱스 (1)가 부여된다.\n",
        "tokenizer.fit_on_texts(sentences_1)                           # (인코딩) 입력되는 문장들 내의 단어들에 인덱스를 부여한다. 구두점 (, . ! ?)은 제외된다.\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "sentences_1_seq = tokenizer.texts_to_sequences(sentences_1)\n",
        "padding = pad_sequences(sentences_1_seq, padding = 'post')        # setences_1_seq의 가장 긴 길이의 문장에 따라 패딩을 설정한다.\n",
        "\n",
        "sentences_2_seq = tokenizer.texts_to_sequences(sentences_2)\n",
        "\n",
        "\n",
        "print(\"sentences_1 : \", sentences_1)\n",
        "print(\"sentences_2 : \", sentences_2)\n",
        "print()\n",
        "\n",
        "print(\"fit word index : \", word_index)\n",
        "print(\"sentence_1 to indices sequence : \", sentences_1_seq)\n",
        "print(\"sentence_2 to indices sequence : \", sentences_2_seq)\n",
        "print()\n",
        "\n",
        "print(\"padding of sentence_1 : \", padding)\n",
        "print()\n",
        "\n",
        "print('Encoding samples')\n",
        "print(sentences_1[0], \" -> \", sentences_1_seq[0])\n",
        "print(sentences_2[0], \" -> \", sentences_2_seq[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaZXn0zVigng",
        "outputId": "12ef8a46-2191-43bc-c0a1-3c09cab44539"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentences_1 :  [\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\", \"One more pseudo generalization or I'm giving up.\"]\n",
            "sentences_2 :  ['The more we study verbs, the crazier they get.', 'Day by day the facts are getting murkier.']\n",
            "\n",
            "fit word index :  {'<OOV>': 1, 'one': 2, 'more': 3, 'pseudo': 4, 'generalization': 5, \"i'm\": 6, 'giving': 7, 'up': 8, 'our': 9, 'friends': 10, \"won't\": 11, 'buy': 12, 'this': 13, 'analysis': 14, 'let': 15, 'alone': 16, 'the': 17, 'next': 18, 'we': 19, 'propose': 20, 'and': 21, 'or': 22}\n",
            "sentence_1 to indices sequence :  [[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 2, 19, 20], [2, 3, 4, 5, 21, 6, 7, 8], [2, 3, 4, 5, 22, 6, 7, 8]]\n",
            "sentence_2 to indices sequence :  [[17, 3, 19, 1, 1, 17, 1, 1, 1], [1, 1, 1, 17, 1, 1, 1, 1]]\n",
            "\n",
            "padding of sentence_1 :  [[ 9 10 11 12 13 14 15 16 17 18  2 19 20]\n",
            " [ 2  3  4  5 21  6  7  8  0  0  0  0  0]\n",
            " [ 2  3  4  5 22  6  7  8  0  0  0  0  0]]\n",
            "\n",
            "Encoding samples\n",
            "Our friends won't buy this analysis, let alone the next one we propose.  ->  [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 2, 19, 20]\n",
            "The more we study verbs, the crazier they get.  ->  [17, 3, 19, 1, 1, 17, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B7Yp4L6goWC1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}